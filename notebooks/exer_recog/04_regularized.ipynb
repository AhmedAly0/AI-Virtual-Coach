{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc5daa1",
   "metadata": {},
   "source": [
    "## Architecture-Specific Head Designs (v4) - Regularized Dual-Pooling\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Dual Pooling: GAP + GMP Concatenated**\n",
    "   - Captures both average patterns (GAP) and salient features (GMP)\n",
    "   - Richer feature representation than single pooling\n",
    "   - Doubles feature dimension after concatenation\n",
    "\n",
    "2. **Smaller classification heads**\n",
    "   - Reduces overfitting risk compared to v3\n",
    "   - Forces model to learn better representations in backbone\n",
    "\n",
    "3. **Label smoothing (0.05)**\n",
    "   - Softens target labels: correct class gets 0.95, others share 0.05\n",
    "   - Improves generalization and calibration\n",
    "   - Prevents overconfident predictions\n",
    "\n",
    "4. **AdamW optimizer with weight decay**\n",
    "   - Weight decay (1e-4) instead of L2 regularization\n",
    "   - Better generalization than standard Adam\n",
    "   - Decouples weight decay from gradient updates\n",
    "\n",
    "5. **Differential learning rates**\n",
    "   - Classification head: 1e-4 (higher LR for new layers)\n",
    "   - Backbone: 1e-5 (lower LR to preserve pretrained features)\n",
    "   - Prevents catastrophic forgetting during fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "### Head Architectures (v4)\n",
    "\n",
    "**EfficientNet Family (B0, B2, B3):**\n",
    "```\n",
    "Dual Pooling (GAP + GMP concatenated)\n",
    "    ‚Üì\n",
    "BatchNormalization\n",
    "    ‚Üì\n",
    "Dense(256, activation='swish')\n",
    "    ‚Üì\n",
    "Dropout(0.3)\n",
    "    ‚Üì\n",
    "Dense(128, activation='swish')\n",
    "    ‚Üì\n",
    "Dropout(0.2)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "- **Regularization:** BatchNorm + progressive dropout (0.3 ‚Üí 0.2)\n",
    "- **Size:** 256‚Üí128 (smaller than v3's 512‚Üí256)\n",
    "\n",
    "**ResNet50:**\n",
    "```\n",
    "Dual Pooling (GAP + GMP concatenated)\n",
    "    ‚Üì\n",
    "BatchNormalization\n",
    "    ‚Üì\n",
    "Dense(256, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.4)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "- **Regularization:** Single hidden layer with higher dropout (0.4)\n",
    "- **Size:** 256 (reduced from v3's 1024‚Üí512)\n",
    "\n",
    "**VGG16:**\n",
    "```\n",
    "Dual Pooling (GAP + GMP concatenated)\n",
    "    ‚Üì\n",
    "BatchNormalization\n",
    "    ‚Üì\n",
    "Dense(256, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.4)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "- **Regularization:** Same as ResNet50 (single layer + high dropout)\n",
    "- **Size:** 256 (reduced from v3's 512‚Üí256)\n",
    "\n",
    "**MobileNet Family (V2, V3-Large):**\n",
    "```\n",
    "Dual Pooling (GAP + GMP concatenated)\n",
    "    ‚Üì\n",
    "Dense(128, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.25)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "- **Regularization:** Single hidden layer, moderate dropout\n",
    "- **Size:** 128 (compact to match lightweight backbone)\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences from Experiment 3 (v3)\n",
    "\n",
    "| Aspect | Experiment 3 (v3) | Experiment 4 (v4) |\n",
    "|--------|------------------|------------------|\n",
    "| **Pooling** | Single GAP | Dual GAP + GMP |\n",
    "| **Head Size** | Larger (512‚Üí256, 1024‚Üí512) | Smaller (256‚Üí128, 256 only) |\n",
    "| **Label Smoothing** | None (0.0) | 0.05 |\n",
    "| **Optimizer** | Adam | AdamW (weight decay 1e-4) |\n",
    "| **Learning Rates** | Single LR (1e-3, 1e-4) | Differential (head: 1e-4, backbone: 1e-5) |\n",
    "| **Unfreezing** | Architecture-specific % | Uniform 50% for all |\n",
    "\n",
    "**Hypothesis:** Stronger regularization (smaller heads, label smoothing, weight decay) will:\n",
    "- ‚úÖ Reduce overfitting\n",
    "- ‚úÖ Improve generalization (higher test accuracy)\n",
    "- ‚úÖ Increase consistency (lower std deviation)\n",
    "- ‚úÖ Better calibrated predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82bdf0",
   "metadata": {},
   "source": [
    "## Training Strategy (3-Way Split with Validation Monitoring)\n",
    "\n",
    "### Data Split (Subject-Independent)\n",
    "- **Training set (~55% of subjects)**: Used for model learning\n",
    "- **Validation set (~15% of subjects)**: Used for early stopping and learning rate scheduling\n",
    "- **Test set (~30% of subjects)**: Used ONLY for final evaluation (never seen during training)\n",
    "\n",
    "‚úÖ **No Data Leakage:** All samples from a subject stay in the same group (train/val/test)\n",
    "\n",
    "### Phase 1: Frozen Backbone (up to 30 epochs)\n",
    "- All backbone layers frozen\n",
    "- Train only dual-pooling classification head\n",
    "- Learning rate: 0.0001 (1e-4)\n",
    "- Optimizer: **AdamW** with weight decay 1e-4\n",
    "- **Callbacks:** EarlyStopping (patience=10), ReduceLROnPlateau, ModelCheckpoint\n",
    "- **Monitoring:** Validation loss (stops when validation stops improving)\n",
    "\n",
    "### Phase 2: Progressive Unfreezing (up to 20 epochs)\n",
    "- Unfreeze top 50% of backbone layers (uniform for all architectures)\n",
    "- Learning rate: 0.00001 (1e-5) - 10√ó reduction\n",
    "- Optimizer: **AdamW** with weight decay 1e-4\n",
    "- **Same callbacks** with validation monitoring\n",
    "- Differential LR approximated via lower global LR\n",
    "\n",
    "### Data Augmentation (Basic - Same as Exp 3)\n",
    "- Horizontal flip: `True`\n",
    "- Translation: ¬±15% (height + width)\n",
    "- **No rotation/zoom/brightness** (keep it simple)\n",
    "\n",
    "### Regularization Techniques Applied\n",
    "1. **Dual pooling** (GAP + GMP) - Richer features\n",
    "2. **Smaller heads** - Less overfitting\n",
    "3. **Label smoothing (0.05)** - Better generalization\n",
    "4. **AdamW optimizer** - Weight decay regularization\n",
    "5. **Differential LRs** - Preserve pretrained features\n",
    "6. **Progressive dropout** - Higher ‚Üí lower through layers\n",
    "7. **BatchNormalization** - Stable training\n",
    "\n",
    "### Evaluation Protocol\n",
    "- **5 runs per backbone** (quick validation phase)\n",
    "- Subject-independent 3-way split (55%/15%/30%)\n",
    "- Test set evaluated ONLY at the end (after all training completes)\n",
    "- Metrics: Training accuracy, validation accuracy, test accuracy\n",
    "- **Success criteria compared to Experiment 3:**\n",
    "  - Improved: Higher mean test accuracy\n",
    "  - More stable: Lower std deviation\n",
    "  - Better calibration: Reduced validation-test gap\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14175938",
   "metadata": {},
   "source": [
    "## Setup: TensorFlow Configuration\n",
    "\n",
    "This cell configures TensorFlow to suppress warnings and enable GPU memory growth.\n",
    "\n",
    "**Key configurations:**\n",
    "- Suppress TensorFlow/CUDA warnings for cleaner output\n",
    "- Enable GPU memory growth (prevents out-of-memory errors)\n",
    "- Set logging levels to ERROR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01de0f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow configured:\n",
      "   TF_CPP_MIN_LOG_LEVEL: 3\n",
      "   AUTOGRAPH_VERBOSITY: 0\n",
      "   TensorFlow version: 2.10.0\n",
      "   GPUs detected: 1\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Run this cell FIRST before any other imports\n",
    "# Suppress TensorFlow warnings at the OS level before TensorFlow loads\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set environment variables BEFORE TensorFlow is imported anywhere\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=filter INFO, 2=filter WARNING, 3=errors only\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'   # Disable AutoGraph conversion warnings\n",
    "\n",
    "# Filter Python warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Suppress absl logging (used by TensorFlow internally)\n",
    "try:\n",
    "    from absl import logging as absl_logging\n",
    "    absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Redirect stderr temporarily to suppress any remaining warnings during TF import\n",
    "stderr_backup = sys.stderr\n",
    "sys.stderr = io.StringIO()\n",
    "\n",
    "# Restore stderr\n",
    "sys.stderr = stderr_backup\n",
    "\n",
    "# Final TensorFlow logging configuration\n",
    "try:\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Enable GPU memory growth\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ TensorFlow configured:\")\n",
    "print(\"   TF_CPP_MIN_LOG_LEVEL:\", os.environ.get('TF_CPP_MIN_LOG_LEVEL'))\n",
    "print(\"   AUTOGRAPH_VERBOSITY:\", os.environ.get('AUTOGRAPH_VERBOSITY'))\n",
    "print(\"   TensorFlow version:\", tf.__version__)\n",
    "print(\"   GPUs detected:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d369e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: d:\\Graduation_Project\\ai-virtual-coach\n",
      "‚úÖ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import logging\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from src.data import load_front_side_geis, get_subjects_identities\n",
    "from src.scripts.experiment_4 import train_experiment_4\n",
    "from src.utils.io_utils import load_config\n",
    "from src.utils.metrics import load_backbone_results_with_config\n",
    "\n",
    "# Configure logging\n",
    "root_logger = logging.getLogger()\n",
    "if root_logger.hasHandlers():\n",
    "    for handler in root_logger.handlers[:]:\n",
    "        root_logger.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(\"‚úÖ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63162e20",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3fea2",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Use the shared `load_front_side_geis` helper to merge both camera views with a reproducible shuffle driven by the configuration seed.\n",
    "\n",
    "‚ÑπÔ∏è **New:** Experiment 4 now consumes these samples through the streaming tf.data helpers (Option B), so the heavy preprocessing happens on-the-fly with minimal RAM pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82761910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from: d:\\Graduation_Project\\ai-virtual-coach\\config\\experiment_4.yaml\n",
      "\n",
      "Key parameters:\n",
      "  Strategy         : 2-phase-validation-monitored-regularized-dual-pooling\n",
      "  Frozen epochs    : 30\n",
      "  Fine-tune epochs : 20\n",
      "  Batch size       : 32\n",
      "  Label smoothing  : 0.05\n",
      "  Random seed      : 42\n",
      "\n",
      "Unfreezing strategy (Phase 2):\n"
     ]
    }
   ],
   "source": [
    "# Load YAML configuration shared across Experiment 4 pipelines\n",
    "CONFIG_PATH = project_root / 'config' / 'experiment_4.yaml'\n",
    "config_path = CONFIG_PATH  # Preserve legacy variable name for downstream cells\n",
    "config = load_config(str(CONFIG_PATH))\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded from: {CONFIG_PATH}\")\n",
    "print(\"\\nKey parameters:\")\n",
    "print(f\"  Strategy         : {config['training']['strategy']}\")\n",
    "print(f\"  Frozen epochs    : {config['training']['frozen_epochs']}\")\n",
    "print(f\"  Fine-tune epochs : {config['training']['fine_tune_epochs']}\")\n",
    "print(f\"  Batch size       : {config['training']['batch_size']}\")\n",
    "print(f\"  Label smoothing  : {config['model']['label_smoothing']}\")\n",
    "print(f\"  Random seed      : {config['random_seed']}\")\n",
    "\n",
    "print(\"\\nUnfreezing strategy (Phase 2):\")\n",
    "#for backbone, params in config['unfreezing'].items():\n",
    "    #pct = params['phase2_unfreeze_percent'] * 100\n",
    "    #print(f\"  {backbone:<18} ‚Üí {pct:>4.0f}% of layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4d41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GEIs from:\n",
      "  Front: d:\\Graduation_Project\\ai-virtual-coach\\datasets\\GEIs_of_rgb_front\\GEIs\n",
      "  Side : d:\\Graduation_Project\\ai-virtual-coach\\datasets\\GEIs_of_rgb_side\\GEIs\n",
      "‚úÖ Dataset loaded: 3142 samples (front: 1574, side: 1568)\n",
      "Total unique subjects: 70\n",
      "Subject preview: ['V3', 'V31', 'V39', 'V4', 'V46', 'V47', 'V48', 'V5', 'V50', 'Volunteer #1']\n",
      "   Sample structure: (label:str, image:np.ndarray, subject:str)\n",
      "   Sample types: str, (1280, 720), str\n",
      "‚úÖ Dataset loaded: 3142 samples (front: 1574, side: 1568)\n",
      "Total unique subjects: 70\n",
      "Subject preview: ['V3', 'V31', 'V39', 'V4', 'V46', 'V47', 'V48', 'V5', 'V50', 'Volunteer #1']\n",
      "   Sample structure: (label:str, image:np.ndarray, subject:str)\n",
      "   Sample types: str, (1280, 720), str\n"
     ]
    }
   ],
   "source": [
    "# Load datasets using shared helper (front + side views)\n",
    "front_base_folder = str(project_root / 'datasets' / 'GEIs_of_rgb_front' / 'GEIs')\n",
    "side_base_folder = str(project_root / 'datasets' / 'GEIs_of_rgb_side' / 'GEIs')\n",
    "\n",
    "print(f\"Loading GEIs from:\\n  Front: {front_base_folder}\\n  Side : {side_base_folder}\")\n",
    "\n",
    "dataset, dataset_summary = load_front_side_geis(\n",
    "    front_base_folder=front_base_folder,\n",
    "    side_base_folder=side_base_folder,\n",
    "    seed=config['random_seed'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"‚úÖ Dataset loaded: {dataset_summary['total_count']} samples \"\n",
    "    f\"(front: {dataset_summary['front_count']}, side: {dataset_summary['side_count']})\"\n",
    ")\n",
    "subjects = get_subjects_identities(dataset)\n",
    "subject_count = len(subjects)\n",
    "\n",
    "\n",
    "print(f'Total unique subjects: {subject_count}')\n",
    "print(f'Subject preview: {subjects[:10]}')\n",
    "\n",
    "if dataset:\n",
    "    sample_label, sample_img, sample_subject = dataset[0]\n",
    "    print(\"   Sample structure: (label:str, image:np.ndarray, subject:str)\")\n",
    "    print(f\"   Sample types: {type(sample_label).__name__}, {sample_img.shape}, {type(sample_subject).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b6954",
   "metadata": {},
   "source": [
    "## Single Run Training (Quick Test)\n",
    "\n",
    "Train one run with EfficientNet-B0 to verify the pipeline works correctly.\n",
    "\n",
    "**What happens:**\n",
    "- Dataset split by subjects (55%/15%/30% train/val/test)\n",
    "- Phase 1: Train frozen backbone (up to 30 epochs with early stopping)\n",
    "- Phase 2: Fine-tune unfrozen top 50% (up to 20 epochs with early stopping)\n",
    "- Save best model and results\n",
    "\n",
    "**Streaming loader reminder:**\n",
    "- The training call now uses the new streaming Option B helpers to keep memory usage in check.\n",
    "- Re-run this single-run smoke test after pulling the changes to confirm the kernel stays stable.\n",
    "\n",
    "**Expected duration:** ~5-10 minutes (depends on early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee767019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting single-run smoke test for efficientnet_b0\n",
      "================================================================================\n",
      "Epoch 1/30\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example: Train one run with EfficientNet-B0\n",
    "backbone = 'efficientnet_b0'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Starting single-run smoke test for {backbone}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "single_run_results = train_experiment_4(\n",
    "    dataset=dataset,\n",
    "    backbones=[backbone],\n",
    "    config_path=str(config_path),\n",
    "    num_runs=1\n",
    ")\n",
    "\n",
    "run_metrics = single_run_results[backbone][0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train accuracy: {run_metrics['train_acc']:.4f}\")\n",
    "print(f\"Val accuracy (frozen): {run_metrics['val_acc_frozen']:.4f}\")\n",
    "print(f\"Val accuracy (unfrozen): {run_metrics['val_acc_unfrozen']:.4f}\")\n",
    "print(f\"Test accuracy: {run_metrics['test_acc']:.4f}\")\n",
    "print(f\"Test loss: {run_metrics['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084f147",
   "metadata": {},
   "source": [
    "## Full Experiment Execution (Multiple Backbones √ó 5 Runs)\n",
    "\n",
    "**What's different from Experiment 3:**\n",
    "- Uses v4 architecture with dual pooling (GAP + GMP)\n",
    "- Smaller classification heads (less overfitting)\n",
    "- Label smoothing (0.05) for better generalization\n",
    "- AdamW optimizer with weight decay (1e-4)\n",
    "- Uniform 50% unfreezing for all backbones\n",
    "\n",
    "**Expected duration:** ~1-2 hours for 5 backbones √ó 5 runs = 25 training runs\n",
    "\n",
    "**Backbones tested:**\n",
    "1. EfficientNet B0 - Lightweight, efficient\n",
    "2. EfficientNet B2 - Balanced performance\n",
    "3. ResNet50 - Deep residual architecture\n",
    "4. VGG16 - Classic deep CNN\n",
    "5. MobileNet V2 - Mobile-optimized\n",
    "\n",
    "‚ö†Ô∏è **Note:** This cell will take significant time. Use GPU if available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c929de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure full benchmark sweep\n",
    "BACKBONES_TO_TEST = [\n",
    "    'efficientnet_b0',\n",
    "    'efficientnet_b2',\n",
    "    'resnet50',\n",
    "    'vgg16',\n",
    "    'mobilenet_v2'\n",
    "]\n",
    "N_RUNS = 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 4: Regularized Dual-Pooling Heads\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Backbones: {BACKBONES_TO_TEST}\")\n",
    "print(f\"Runs per backbone: {N_RUNS}\")\n",
    "print(f\"Total training runs: {len(BACKBONES_TO_TEST) * N_RUNS}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "exp4_run_results = train_experiment_4(\n",
    "    dataset=dataset,\n",
    "    backbones=BACKBONES_TO_TEST,\n",
    "    config_path=str(config_path),\n",
    "    num_runs=N_RUNS\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EXPERIMENT 4 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Results saved to: experiments/exer_recog/results/exp_04_regularized/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cf924",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Load and analyze results from all training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment 4 results\n",
    "results_base_dir = project_root / 'experiments' / 'exer_recog' / 'results' / 'exp_04_regularized'\n",
    "exp4_results = load_backbone_results_with_config(results_base_dir=str(results_base_dir))\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 4 RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for backbone in BACKBONES_TO_TEST:\n",
    "    if backbone in exp4_results:\n",
    "        runs = exp4_results[backbone]\n",
    "        test_accs = [r['test_acc'] for r in runs]\n",
    "        val_accs = [r.get('val_acc_unfrozen', r.get('val_acc', 0)) for r in runs]\n",
    "        \n",
    "        print(f\"\\n{backbone}:\")\n",
    "        print(f\"  Mean test accuracy: {np.mean(test_accs):.4f} ¬± {np.std(test_accs):.4f}\")\n",
    "        print(f\"  Best test accuracy: {np.max(test_accs):.4f}\")\n",
    "        print(f\"  Worst test accuracy: {np.min(test_accs):.4f}\")\n",
    "        print(f\"  Mean val accuracy: {np.mean(val_accs):.4f}\")\n",
    "        print(f\"  Number of runs: {len(runs)}\")\n",
    "    else:\n",
    "        print(f\"\\n{backbone}: No results found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34438c",
   "metadata": {},
   "source": [
    "## Visualization: Experiment 4 Performance\n",
    "\n",
    "Bar plot showing mean test accuracy with error bars for each backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3868a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "backbones_list = []\n",
    "mean_accs = []\n",
    "std_accs = []\n",
    "\n",
    "for backbone in BACKBONES_TO_TEST:\n",
    "    if backbone in exp4_results:\n",
    "        runs = exp4_results[backbone]\n",
    "        test_accs = [r['test_acc'] for r in runs]\n",
    "        \n",
    "        backbones_list.append(backbone)\n",
    "        mean_accs.append(np.mean(test_accs))\n",
    "        std_accs.append(np.std(test_accs))\n",
    "\n",
    "# Bar plot with error bars\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(backbones_list)), mean_accs, yerr=std_accs, \n",
    "               capsize=5, alpha=0.7, color='coral', edgecolor='darkred', linewidth=1.5)\n",
    "plt.xticks(range(len(backbones_list)), backbones_list, rotation=45, ha='right')\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Experiment 4: Regularized Dual-Pooling Heads (Mean ¬± Std)', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(mean_accs, std_accs)):\n",
    "    plt.text(i, mean + std + 0.02, f'{mean:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best performer\n",
    "if backbones_list:\n",
    "    best_idx = np.argmax(mean_accs)\n",
    "    print(f\"\\nüèÜ Best performing backbone: {backbones_list[best_idx]}\")\n",
    "    print(f\"   Mean test accuracy: {mean_accs[best_idx]:.4f} ¬± {std_accs[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b78a7",
   "metadata": {},
   "source": [
    "## Compare with Experiment 3 (Smart Heads)\n",
    "\n",
    "Load Experiment 3 results and compare with Experiment 4 to evaluate the impact of regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment 3 results for comparison\n",
    "exp3_base_dir = project_root / 'experiments' / 'exer_recog' / 'results' / 'exp_03_smart_heads'\n",
    "\n",
    "try:\n",
    "    exp3_results = load_backbone_results_with_config(results_base_dir=str(exp3_base_dir))\n",
    "    print(\"‚úÖ Experiment 3 results loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Experiment 3 results: {e}\")\n",
    "    exp3_results = {}\n",
    "\n",
    "# Comparison bar plot\n",
    "if exp3_results and exp4_results:\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    x = np.arange(len(backbones_list))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Calculate means for both experiments\n",
    "    exp3_means = []\n",
    "    exp4_means = []\n",
    "    \n",
    "    for backbone in backbones_list:\n",
    "        if backbone in exp3_results:\n",
    "            exp3_accs = [r['test_acc'] for r in exp3_results[backbone]]\n",
    "            exp3_means.append(np.mean(exp3_accs))\n",
    "        else:\n",
    "            exp3_means.append(0)\n",
    "        \n",
    "        if backbone in exp4_results:\n",
    "            exp4_accs = [r['test_acc'] for r in exp4_results[backbone]]\n",
    "            exp4_means.append(np.mean(exp4_accs))\n",
    "        else:\n",
    "            exp4_means.append(0)\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, exp3_means, width, label='Exp 3: Smart Heads', \n",
    "                   alpha=0.8, color='steelblue', edgecolor='darkblue', linewidth=1.5)\n",
    "    bars2 = ax.bar(x + width/2, exp4_means, width, label='Exp 4: Regularized', \n",
    "                   alpha=0.8, color='coral', edgecolor='darkred', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Backbone', fontsize=12)\n",
    "    ax.set_ylabel('Mean Test Accuracy', fontsize=12)\n",
    "    ax.set_title('Experiment 3 vs Experiment 4: Mean Test Accuracy Comparison', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(backbones_list, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_improvement = 0\n",
    "    num_improved = 0\n",
    "    \n",
    "    for i, backbone in enumerate(backbones_list):\n",
    "        if exp3_means[i] > 0 and exp4_means[i] > 0:\n",
    "            improvement = exp4_means[i] - exp3_means[i]\n",
    "            total_improvement += improvement\n",
    "            if improvement > 0:\n",
    "                num_improved += 1\n",
    "            print(f\"{backbone}: {exp3_means[i]:.4f} ‚Üí {exp4_means[i]:.4f} ({improvement:+.4f})\")\n",
    "    \n",
    "    print(f\"\\nBackbones improved: {num_improved}/{len(backbones_list)}\")\n",
    "    print(f\"Average improvement: {total_improvement/len(backbones_list):+.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot compare - missing results from one or both experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc465a4d",
   "metadata": {},
   "source": [
    "## Statistical Comparison: Variance Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard deviations (variance reduction check)\n",
    "print(\"\\nVariance Comparison (Exp 3 vs Exp 4):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Backbone':<20} {'Exp 3 Std':<15} {'Exp 4 Std':<15} {'Change'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for backbone in backbones_list:\n",
    "    if exp3_results[backbone]:\n",
    "        exp3_std = exp3_results[backbone]['std_test_acc']\n",
    "        exp4_std = exp4_results[backbone]['std_test_acc']\n",
    "        change = ((exp4_std - exp3_std) / exp3_std) * 100\n",
    "        \n",
    "        print(f\"{backbone:<20} {exp3_std:<15.4f} {exp4_std:<15.4f} {change:+.1f}%\")\n",
    "\n",
    "# Overall statistics\n",
    "if all(exp3_results[b] for b in backbones_list):\n",
    "    avg_exp3_std = np.mean([exp3_results[b]['std_test_acc'] for b in backbones_list])\n",
    "    avg_exp4_std = np.mean([exp4_results[b]['std_test_acc'] for b in backbones_list])\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Average':<20} {avg_exp3_std:<15.4f} {avg_exp4_std:<15.4f} \"\n",
    "          f\"{((avg_exp4_std - avg_exp3_std) / avg_exp3_std) * 100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd1603",
   "metadata": {},
   "source": [
    "## Box Plot: Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e236b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot comparing distributions\n",
    "fig, axes = plt.subplots(1, len(backbones_list), figsize=(16, 5), sharey=True)\n",
    "\n",
    "for i, backbone in enumerate(backbones_list):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    data_to_plot = []\n",
    "    labels = []\n",
    "    \n",
    "    if exp3_results[backbone]:\n",
    "        data_to_plot.append(exp3_results[backbone]['test_acc_values'])\n",
    "        labels.append('Exp 3')\n",
    "    \n",
    "    data_to_plot.append(exp4_results[backbone]['test_acc_values'])\n",
    "    labels.append('Exp 4')\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    \n",
    "    # Color boxes\n",
    "    colors = ['steelblue', 'coral']\n",
    "    for patch, color in zip(bp['boxes'], colors[-len(data_to_plot):]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(backbone, fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Test Accuracy')\n",
    "\n",
    "fig.suptitle('Distribution Comparison: Experiment 3 vs Experiment 4', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c31e8",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97453bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 4 FINAL SUMMARY: Regularized Dual-Pooling Heads\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if exp4_results:\n",
    "    # Best performing backbone\n",
    "    best_backbone = None\n",
    "    best_mean = 0\n",
    "    best_std = 0\n",
    "    \n",
    "    for backbone in BACKBONES_TO_TEST:\n",
    "        if backbone in exp4_results:\n",
    "            runs = exp4_results[backbone]\n",
    "            test_accs = [r['test_acc'] for r in runs]\n",
    "            mean_acc = np.mean(test_accs)\n",
    "            \n",
    "            if mean_acc > best_mean:\n",
    "                best_mean = mean_acc\n",
    "                best_std = np.std(test_accs)\n",
    "                best_backbone = backbone\n",
    "    \n",
    "    print(f\"\\n1. Best Performing Backbone: {best_backbone}\")\n",
    "    print(f\"   Mean test accuracy: {best_mean:.4f} ¬± {best_std:.4f}\")\n",
    "    \n",
    "    # Comparison with Experiment 3\n",
    "    if exp3_results:\n",
    "        print(f\"\\n2. Comparison with Experiment 3:\")\n",
    "        \n",
    "        improvements = []\n",
    "        for backbone in backbones_list:\n",
    "            if backbone in exp3_results and backbone in exp4_results:\n",
    "                exp3_accs = [r['test_acc'] for r in exp3_results[backbone]]\n",
    "                exp4_accs = [r['test_acc'] for r in exp4_results[backbone]]\n",
    "                improvements.append(np.mean(exp4_accs) - np.mean(exp3_accs))\n",
    "        \n",
    "        if improvements:\n",
    "            avg_improvement = np.mean(improvements)\n",
    "            num_improved = sum(1 for imp in improvements if imp > 0)\n",
    "            \n",
    "            print(f\"   Average accuracy change: {avg_improvement:+.4f}\")\n",
    "            print(f\"   Backbones improved: {num_improved}/{len(improvements)}\")\n",
    "            \n",
    "            # Variance comparison\n",
    "            exp3_stds = []\n",
    "            exp4_stds = []\n",
    "            \n",
    "            for backbone in backbones_list:\n",
    "                if backbone in exp3_results and backbone in exp4_results:\n",
    "                    exp3_accs = [r['test_acc'] for r in exp3_results[backbone]]\n",
    "                    exp4_accs = [r['test_acc'] for r in exp4_results[backbone]]\n",
    "                    exp3_stds.append(np.std(exp3_accs))\n",
    "                    exp4_stds.append(np.std(exp4_accs))\n",
    "            \n",
    "            if exp3_stds and exp4_stds:\n",
    "                avg_exp3_std = np.mean(exp3_stds)\n",
    "                avg_exp4_std = np.mean(exp4_stds)\n",
    "                std_reduction = ((avg_exp4_std - avg_exp3_std) / avg_exp3_std) * 100\n",
    "                \n",
    "                print(f\"   Average variance change: {std_reduction:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\n3. Regularization Techniques Applied:\")\n",
    "    print(f\"   ‚úì Dual pooling (GAP + GMP concatenated)\")\n",
    "    print(f\"   ‚úì Smaller classification heads (reduced capacity)\")\n",
    "    print(f\"   ‚úì Label smoothing ({config['model']['label_smoothing']})\")\n",
    "    print(f\"   ‚úì AdamW optimizer (weight decay: {config['training']['weight_decay']})\")\n",
    "    print(f\"   ‚úì Differential learning rates (head: {config['training']['initial_lr']}, backbone: {config['training']['fine_tune_lr']})\")\n",
    "    print(f\"   ‚úì Progressive dropout (higher ‚Üí lower through layers)\")\n",
    "    print(f\"   ‚úì BatchNormalization for stable training\")\n",
    "    \n",
    "    # Success criteria evaluation\n",
    "    print(f\"\\n4. Success Criteria Evaluation:\")\n",
    "    \n",
    "    if best_mean > 0.88 and best_std < 0.02:\n",
    "        print(\"   ‚úÖ EXCELLENT SUCCESS: Mean > 88% and std < 2%\")\n",
    "        print(\"      ‚Üí Strong regularization achieved both high accuracy and stability\")\n",
    "        print(\"      ‚Üí Ready for production deployment\")\n",
    "    elif best_mean > 0.86 and best_std < 0.025:\n",
    "        print(\"   ‚úÖ GOOD SUCCESS: Mean > 86% and std < 2.5%\")\n",
    "        print(\"      ‚Üí Regularization techniques are effective\")\n",
    "        print(\"      ‚Üí Consider scaling to more runs for best performers\")\n",
    "    elif best_mean > 0.84:\n",
    "        print(\"   ‚úÖ MINIMUM SUCCESS: Mean > 84%\")\n",
    "        print(\"      ‚Üí Regularization shows promise but may need refinement\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è BELOW TARGET: Mean ‚â§ 84%\")\n",
    "        print(\"      ‚Üí Regularization may be too strong (underfitting)\")\n",
    "        print(\"      ‚Üí Consider relaxing some constraints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results saved to: experiments/exer_recog/results/exp_04_regularized/\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c8dd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 4 Complete! üéâ\n",
    "\n",
    "### Results Summary\n",
    "All results saved to: `experiments/exer_recog/results/exp_04_regularized/`\n",
    "\n",
    "**Generated files:**\n",
    "- Individual backbone folders with run-specific results (`results.yaml`)\n",
    "- Model checkpoints (`best_model.keras`)\n",
    "- Training history and metrics\n",
    "\n",
    "### Key Questions to Answer\n",
    "\n",
    "**1. Did regularization improve performance over Experiment 3?**\n",
    "- Compare mean test accuracy: Exp4 vs Exp3\n",
    "- Check if dual pooling + smaller heads + label smoothing helped\n",
    "\n",
    "**2. Is the model more stable/consistent?**\n",
    "- Compare std deviation: Exp4 vs Exp3\n",
    "- Lower std = more reliable performance across runs\n",
    "\n",
    "**3. Which regularization technique contributed most?**\n",
    "- Dual pooling for richer features?\n",
    "- Label smoothing for calibration?\n",
    "- Weight decay for parameter control?\n",
    "- Smaller heads for reduced overfitting?\n",
    "\n",
    "### Regularization Impact Analysis\n",
    "\n",
    "**If Mean Accuracy Improved AND Std Decreased:**\n",
    "- ‚úÖ Regularization is effective\n",
    "- ‚úÖ Model generalizes better\n",
    "- ‚úÖ More reliable for deployment\n",
    "- ‚Üí **Success!** Consider this architecture for production\n",
    "\n",
    "**If Mean Accuracy Same BUT Std Decreased:**\n",
    "- ‚úÖ More consistent predictions\n",
    "- ‚úÖ Reduced overfitting\n",
    "- ‚Üí **Partial success** - stability improved without accuracy loss\n",
    "\n",
    "**If Mean Accuracy Decreased:**\n",
    "- ‚ùå Over-regularized (underfitting)\n",
    "- ‚Üí Consider relaxing constraints:\n",
    "  - Increase head size\n",
    "  - Reduce dropout rates\n",
    "  - Lower weight decay\n",
    "  - Reduce label smoothing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**If Excellent Success (mean > 88%, std < 2%):**\n",
    "- üöÄ Deploy best backbone for production\n",
    "- üìä Analyze confusion matrix for failure modes\n",
    "- üéØ Consider ensemble of top performers\n",
    "\n",
    "**If Good Success (mean > 86%, std < 2.5%):**\n",
    "- üìà Scale to 30 runs for statistical confidence\n",
    "- üî¨ Ablation study: test each regularization technique individually\n",
    "- üé® Try additional augmentation\n",
    "\n",
    "**If Below Expectations:**\n",
    "- üîÑ Revert to Experiment 3 architecture\n",
    "- üß™ Ablation study: which regularization hurt performance?\n",
    "- üéõÔ∏è Hyperparameter tuning for weight decay, label smoothing\n",
    "\n",
    "### Design Philosophy Validated?\n",
    "\n",
    "This experiment tests whether **strong regularization improves both accuracy and stability**:\n",
    "- ‚úÖ Dual pooling (GAP + GMP)\n",
    "- ‚úÖ Smaller heads (256‚Üí128 vs 512‚Üí256)\n",
    "- ‚úÖ Label smoothing (0.05)\n",
    "- ‚úÖ AdamW with weight decay (1e-4)\n",
    "- ‚úÖ Differential learning rates\n",
    "- ‚úÖ Progressive dropout\n",
    "- ‚úÖ BatchNormalization\n",
    "\n",
    "**Results will confirm:**\n",
    "1. Does dual pooling capture more informative features?\n",
    "2. Do smaller heads prevent overfitting?\n",
    "3. Does label smoothing improve calibration?\n",
    "4. Is AdamW superior to Adam for this task?\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison with Previous Experiments:**\n",
    "- **Exp 1 (Baseline):** Universal heads, simple training\n",
    "- **Exp 3 (Smart Heads):** Architecture-specific heads, optimized sizes\n",
    "- **Exp 4 (Regularized):** Dual pooling, aggressive regularization, AdamW\n",
    "\n",
    "The progression shows increasingly sophisticated architectures and training strategies! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
