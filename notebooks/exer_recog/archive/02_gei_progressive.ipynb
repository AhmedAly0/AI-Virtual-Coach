{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa961ab6",
   "metadata": {},
   "source": [
    "# Experiment 2: Progressive Training with Optimized Architectures\n",
    "\n",
    "**Goal:** Improve upon baseline results using 3-stage progressive unfreezing with architecture-specific custom heads.\n",
    "\n",
    "## Experiment Design\n",
    "\n",
    "### Key Differences from Baseline\n",
    "1. **Architecture-specific heads:** Custom classification heads tailored to each backbone family\n",
    "2. **Progressive unfreezing:** 3 stages instead of 2 phases\n",
    "3. **More granular fine-tuning:** Gradual unfreezing of backbone layers\n",
    "\n",
    "### Training Strategy (3-Stage Progressive Unfreezing)\n",
    "\n",
    "**Stage 1: Frozen Backbone (10 epochs)**\n",
    "- All backbone layers frozen\n",
    "- Train only custom classification head\n",
    "- Learning rate: 0.001\n",
    "- Optimizer: Adam\n",
    "\n",
    "**Stage 2: Partial Unfreezing (10 epochs)**\n",
    "- Unfreeze last N layers (architecture-dependent)\n",
    "  - EfficientNet: Last 20 layers\n",
    "  - ResNet50: Last 10 layers  \n",
    "  - VGG16: Last 4 layers\n",
    "  - MobileNet: Last 15 layers\n",
    "- Learning rate: 0.0001 (10√ó reduction)\n",
    "- Fine-tune high-level features\n",
    "\n",
    "**Stage 3: Full Fine-tuning (10 epochs)**\n",
    "- Unfreeze entire backbone\n",
    "- Learning rate: 0.00001 (100√ó reduction from Stage 1)\n",
    "- Full end-to-end training\n",
    "\n",
    "### Architecture-Specific Custom Heads\n",
    "\n",
    "**EfficientNet Family (B0, B2, B3):**\n",
    "```\n",
    "GlobalAveragePooling2D\n",
    "    ‚Üì\n",
    "Dense(256, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.5)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "\n",
    "**ResNet50:**\n",
    "```\n",
    "GlobalAveragePooling2D\n",
    "    ‚Üì\n",
    "Dense(512, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.4)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "\n",
    "**VGG16:**\n",
    "```\n",
    "Flatten\n",
    "    ‚Üì\n",
    "Dense(1024, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.5)\n",
    "    ‚Üì\n",
    "Dense(512, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.5)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "\n",
    "**MobileNet Family (V2, V3-Large):**\n",
    "```\n",
    "GlobalAveragePooling2D\n",
    "    ‚Üì\n",
    "Dense(128, activation='relu')\n",
    "    ‚Üì\n",
    "Dropout(0.3)\n",
    "    ‚Üì\n",
    "Dense(num_classes, activation='softmax')\n",
    "```\n",
    "\n",
    "### Hypothesis\n",
    "Progressive unfreezing with architecture-specific heads should:\n",
    "- ‚úÖ Prevent catastrophic forgetting\n",
    "- ‚úÖ Allow better feature adaptation\n",
    "- ‚úÖ Achieve higher test accuracy than baseline\n",
    "- ‚úÖ Show more stable training curves\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Run this cell FIRST before any other imports\n",
    "# Suppress TensorFlow warnings at the OS level before TensorFlow loads\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set environment variables BEFORE TensorFlow is imported anywhere\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=filter INFO, 2=filter WARNING, 3=errors only\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'   # Disable AutoGraph conversion warnings\n",
    "\n",
    "# Filter Python warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Suppress absl logging (used by TensorFlow internally)\n",
    "try:\n",
    "    from absl import logging as absl_logging\n",
    "    absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Redirect stderr temporarily to suppress any remaining warnings during TF import\n",
    "stderr_backup = sys.stderr\n",
    "sys.stderr = io.StringIO()\n",
    "\n",
    "# Restore stderr\n",
    "sys.stderr = stderr_backup\n",
    "\n",
    "# Final TensorFlow logging configuration\n",
    "try:\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Enable GPU memory growth\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ TensorFlow imported with all warnings suppressed\")\n",
    "print(\"   TF_CPP_MIN_LOG_LEVEL:\", os.environ.get('TF_CPP_MIN_LOG_LEVEL'))\n",
    "print(\"   AUTOGRAPH_VERBOSITY:\", os.environ.get('AUTOGRAPH_VERBOSITY'))\n",
    "print(\"   TensorFlow version:\", tf.__version__)\n",
    "print(\"   GPUs detected:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacfa08",
   "metadata": {},
   "source": [
    "## Setup: TensorFlow Configuration\n",
    "\n",
    "Configure TensorFlow environment before any heavy imports.\n",
    "\n",
    "**Critical settings:**\n",
    "- GPU memory growth enabled (prevents OOM errors during long training)\n",
    "- All TensorFlow warnings suppressed\n",
    "- Logging level set to ERROR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project modules\n",
    "from src.data import load_front_side_geis\n",
    "from src.scripts.experiment_4 import train_experiment_4\n",
    "from src.utils.io_utils import load_config\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"‚úÖ All modules imported successfully from refactored structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8a03f",
   "metadata": {},
   "source": [
    "## Import Refactored Modules\n",
    "\n",
    "Import modular components from the refactored codebase.\n",
    "\n",
    "**Key changes vs the original implementation:**\n",
    "- Shared dataset loading through `load_front_side_geis`\n",
    "- Centralized training via `train_experiment_4` (no custom loops here)\n",
    "- Minimal notebook-side state (all logging, results, and summaries handled by pipelines)\n",
    "\n",
    "**Memory management is still critical** for this experiment due to:\n",
    "- Longer training (3 stages vs 2 phases)\n",
    "- Multiple runs back-to-back\n",
    "- Large models (some >100M parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbc4a0",
   "metadata": {},
   "source": [
    "## Load Datasets (Front + Side Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef01b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML file\n",
    "CONFIG_PATH = project_root / 'config' / 'experiment_4.yaml'\n",
    "config = load_config(str(CONFIG_PATH))\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded from: {CONFIG_PATH}\")\n",
    "print(f\"   Strategy: {config['training']['strategy']}\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Test ratio: {config['dataset']['test_ratio']}\")\n",
    "print(f\"   Number of runs: 10 (hardcoded in notebook)\")\n",
    "\n",
    "# Folder paths - using the datasets folder in the project\n",
    "front_base_folder = str(project_root / 'datasets/GEIs_of_rgb_front/GEIs')\n",
    "side_base_folder = str(project_root / 'datasets/GEIs_of_rgb_side/GEIs')\n",
    "\n",
    "# Load both datasets using shared helper\n",
    "dataset, dataset_summary = load_front_side_geis(\n",
    "    front_base_folder=front_base_folder,\n",
    "    side_base_folder=side_base_folder,\n",
    "    seed=config['random_seed'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nMerged dataset size: {dataset_summary['total_count']} (front: {dataset_summary['front_count']}, side: {dataset_summary['side_count']})\")\n",
    "\n",
    "if dataset:\n",
    "    sample_label, sample_img, sample_subject = dataset[0]\n",
    "    print(f\"Sample tuple structure: (label:str, image:np.ndarray[H,W], subject:str) -> {type(sample_label).__name__} {sample_img.shape} {type(sample_subject).__name__}\")\n",
    "\n",
    "# Get unique labels\n",
    "all_labels = [item[0] for item in dataset]\n",
    "unique_labels = sorted(set(all_labels))\n",
    "print(f\"Number of classes: {len(unique_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b304d7",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load and merge GEI datasets from both camera views.\n",
    "\n",
    "**Same data as Experiment 1** to ensure fair comparison:\n",
    "- Front-view GEIs: Multiple angles of exercises\n",
    "- Side-view GEIs: Complementary perspective\n",
    "- Total: ~1000+ samples across 15 exercise classes\n",
    "- Subject-independent splits (same volunteers never in train + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abe99e",
   "metadata": {},
   "source": [
    "## Train Progressive Models (7 Backbones √ó 10 Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e12004",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"#\" * 80)\n",
    "logger.info(\"EXPERIMENT 2: MULTI-BACKBONE PROGRESSIVE TRAINING (delegates to Experiment 4 pipeline)\")\n",
    "logger.info(\"#\" * 80)\n",
    "\n",
    "BACKBONES_TO_TEST = [\n",
    "    'efficientnet_b0',\n",
    "    'efficientnet_b2',\n",
    "    'efficientnet_b3',\n",
    "    'resnet50',\n",
    "    'vgg16',\n",
    "    'mobilenet_v2',\n",
    "    'mobilenet_v3_large',\n",
    "]\n",
    "\n",
    "N_RUNS = 10  # Number of runs per backbone\n",
    "\n",
    "experiment_results = train_experiment_4(\n",
    "    dataset=dataset,\n",
    "    backbones=BACKBONES_TO_TEST,\n",
    "    config_path=str(CONFIG_PATH),\n",
    "    num_runs=N_RUNS,\n",
    ")\n",
    "\n",
    "comparison_rows = []\n",
    "for backbone, runs in experiment_results.items():\n",
    "    if not runs:\n",
    "        logger.warning(f\"No successful runs recorded for {backbone}\")\n",
    "        continue\n",
    "    test_accs = [run['test_acc'] for run in runs]\n",
    "    comparison_rows.append({\n",
    "        'backbone': backbone,\n",
    "        'mean_test_acc': np.mean(test_accs),\n",
    "        'std_test_acc': np.std(test_accs),\n",
    "        'successful_runs': len(runs),\n",
    "    })\n",
    "\n",
    "if comparison_rows:\n",
    "    comparison_df = pd.DataFrame(comparison_rows).set_index('backbone')\n",
    "    comparison_df = comparison_df.sort_values('mean_test_acc', ascending=False)\n",
    "    display(comparison_df)\n",
    "\n",
    "    os.makedirs(config['results']['base_dir'], exist_ok=True)\n",
    "    csv_path = Path(config['results']['base_dir']) / 'backbone_comparison_exp2.csv'\n",
    "    comparison_df.to_csv(csv_path)\n",
    "    logger.info(f\"\\n‚úì Results saved to: {csv_path}\")\n",
    "else:\n",
    "    logger.error(\"No results to summarize; check earlier logs for failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef054a",
   "metadata": {},
   "source": [
    "## Progressive Training Execution\n",
    "\n",
    "**3-Stage training pipeline (implemented inside `src/Training/experiment_4.py`):**\n",
    "\n",
    "1. **Stage 1 (Frozen):** Train custom head only\n",
    "   - Fast convergence\n",
    "   - Adapts head to dataset\n",
    "   \n",
    "2. **Stage 2 (Partial Unfreeze):** Fine-tune top layers\n",
    "   - Adjusts high-level features\n",
    "   - Maintains pretrained low-level features\n",
    "   \n",
    "3. **Stage 3 (Full Unfreeze):** End-to-end fine-tuning\n",
    "   - Final refinement\n",
    "   - All layers adapt to exercise recognition\n",
    "\n",
    "**What this cell does now:**\n",
    "- Delegates the full sweep to `train_experiment_4`\n",
    "- Reuses the shared dataset already loaded in memory\n",
    "- Saves summaries/CSV outputs through the central pipeline\n",
    "\n",
    "**Expected duration:** ~15-20 minutes per backbone (3 stages √ó 10 epochs each)\n",
    "\n",
    "‚ö†Ô∏è **Note:** Full experiment takes several hours (7 backbones √ó 10 runs = 70 complete 3-stage training pipelines)\n",
    "\n",
    "**Progress tracking:**\n",
    "- Logging comes from the training pipeline (per-run + per-backbone)\n",
    "- Final DataFrame summarises mean/stdev accuracy per backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec96a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 2 Complete! üéâ\n",
    "\n",
    "### Results Summary\n",
    "All artifacts saved to: `experiments/exer_recog/results/exp_04_regularized/` (as configured in `experiment_4.yaml`).\n",
    "\n",
    "**Each backbone folder contains:**\n",
    "- `summary.csv` - Statistics across runs\n",
    "- `all_results.json` - Complete metrics for all runs\n",
    "- `plots/` - Confusion matrices and learning curves\n",
    "- `models/` - Saved model weights (optional)\n",
    "\n",
    "### Next Steps\n",
    "1. **Compare with Baseline:** Open `99_comparison.ipynb` to see side-by-side accuracy comparisons and statistical tests.\n",
    "2. **Analyze Individual Runs:** Inspect saved plots/confusion matrices per backbone.\n",
    "3. **Model Deployment:** Use best performing backbone for production.\n",
    "\n",
    "### Expected Improvements Over Baseline\n",
    "- ‚úÖ Higher mean test accuracy\n",
    "- ‚úÖ Lower standard deviation (more stable)\n",
    "- ‚úÖ Better handling of difficult exercise classes\n",
    "- ‚úÖ Smoother learning curves\n",
    "\n",
    "**Hypothesis validation:** Check if progressive unfreezing + custom heads outperform standard transfer learning!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
