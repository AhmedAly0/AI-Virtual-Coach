{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c55d35",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline Transfer Learning\n",
    "\n",
    "**Goal:** Benchmark 7 pre-trained CNN backbones using standard 2-phase transfer learning to establish baseline performance for exercise recognition from Gait Energy Images (GEIs).\n",
    "\n",
    "## Experiment Design\n",
    "\n",
    "### Dataset\n",
    "- **Modalities:** Front-view + Side-view GEIs (merged)\n",
    "- **Classes:** 15 different exercises\n",
    "- **Split:** 70% training / 30% testing (subject-independent)\n",
    "- **Input:** 224√ó224 grayscale images (converted to RGB for pretrained models)\n",
    "\n",
    "### Training Strategy (2-Phase Transfer Learning)\n",
    "**Phase 1 (Frozen Backbone):**\n",
    "- Freeze all backbone layers\n",
    "- Train only custom classification head\n",
    "- 10 epochs, learning rate: 0.001\n",
    "- Purpose: Adapt the head to our specific task\n",
    "\n",
    "**Phase 2 (Fine-tuning):**\n",
    "- Unfreeze last few layers of backbone\n",
    "- Train with lower learning rate: 0.0001\n",
    "- 10 epochs\n",
    "- Purpose: Fine-tune features for exercise recognition\n",
    "\n",
    "### Backbones Tested\n",
    "1. **EfficientNet-B0** - Lightweight, efficient architecture\n",
    "2. **EfficientNet-B2** - Moderate capacity\n",
    "3. **EfficientNet-B3** - Higher capacity\n",
    "4. **ResNet50** - Classic residual network\n",
    "5. **VGG16** - Deep but simple architecture\n",
    "6. **MobileNet-V2** - Mobile-optimized, efficient\n",
    "7. **MobileNet-V3-Large** - Latest mobile architecture\n",
    "\n",
    "### Evaluation Protocol\n",
    "- **Runs per backbone:** 10 independent runs (different random splits)\n",
    "- **Metrics:** Training accuracy, test accuracy, confusion matrix\n",
    "- **Statistical analysis:** Mean ¬± standard deviation across runs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e40ce2",
   "metadata": {},
   "source": [
    "## Setup: TensorFlow Configuration\n",
    "\n",
    "This cell configures TensorFlow to suppress warnings and enable GPU memory growth.\n",
    "\n",
    "**Key configurations:**\n",
    "- Suppress TensorFlow/CUDA warnings for cleaner output\n",
    "- Enable GPU memory growth (prevents out-of-memory errors)\n",
    "- Set logging levels to ERROR only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30d481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow imported with all warnings suppressed\n",
      "   TF_CPP_MIN_LOG_LEVEL: 3\n",
      "   AUTOGRAPH_VERBOSITY: 0\n",
      "   TensorFlow version: 2.10.0\n",
      "   GPUs detected: 1\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Run this cell FIRST before any other imports\n",
    "# Suppress TensorFlow warnings at the OS level before TensorFlow loads\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import io\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set environment variables BEFORE TensorFlow is imported anywhere\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=filter INFO, 2=filter WARNING, 3=errors only\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'   # Disable AutoGraph conversion warnings\n",
    "\n",
    "# Filter Python warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Suppress absl logging (used by TensorFlow internally)\n",
    "try:\n",
    "    from absl import logging as absl_logging\n",
    "    absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Redirect stderr temporarily to suppress any remaining warnings during TF import\n",
    "stderr_backup = sys.stderr\n",
    "sys.stderr = io.StringIO()\n",
    "\n",
    "# Restore stderr\n",
    "sys.stderr = stderr_backup\n",
    "\n",
    "# Final TensorFlow logging configuration\n",
    "try:\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Enable GPU memory growth\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ TensorFlow imported with all warnings suppressed\")\n",
    "print(\"   TF_CPP_MIN_LOG_LEVEL:\", os.environ.get('TF_CPP_MIN_LOG_LEVEL'))\n",
    "print(\"   AUTOGRAPH_VERBOSITY:\", os.environ.get('AUTOGRAPH_VERBOSITY'))\n",
    "print(\"   TensorFlow version:\", tf.__version__)\n",
    "print(\"   GPUs detected:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7c0c8",
   "metadata": {},
   "source": [
    "## Import Modules\n",
    "\n",
    "**Data modules (`src.data`):**\n",
    "- `load_data` - Load GEI images from folder structure\n",
    "- `split_training_testing_by_subject` - Subject-independent data splitting\n",
    "\n",
    "**Model modules (`src.models`):**\n",
    "- `build_model_for_backbone` - Build transfer learning models\n",
    "\n",
    "**Training modules (`src.Training`):**\n",
    "- `train_one_run` - Execute one complete training run (Phase 1 + Phase 2)\n",
    "\n",
    "**Utility modules (`src.utils`):**\n",
    "- `setup_results_folder_for_backbone` - Organize results by backbone\n",
    "- `save_experiment_summary` - Save metrics and statistics\n",
    "- `get_all_model_parameters` - Count model parameters\n",
    "- `load_backbone_results_with_config` - Load saved results\n",
    "- `create_comprehensive_comparison` - Generate comparison reports\n",
    "- `generate_statistical_comparison` - Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6064f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules imported successfully from refactored structure\n"
     ]
    }
   ],
   "source": [
    "# Import refactored modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('__file__'), '../..')))\n",
    "\n",
    "# Data loading and preprocessing\n",
    "from src.data import load_data, split_training_testing_by_subject, get_subjects_identities\n",
    "\n",
    "# Model building\n",
    "from src.models import build_model_for_backbone\n",
    "\n",
    "# Training experiments\n",
    "from src.scripts import train_one_run\n",
    "\n",
    "# Utilities\n",
    "from src.utils import (\n",
    "    set_global_seed,\n",
    "    setup_results_folder_for_backbone,\n",
    "    save_experiment_summary,\n",
    "    get_all_model_parameters,\n",
    "    load_backbone_results_with_config,\n",
    "    create_comprehensive_comparison,\n",
    "    generate_statistical_comparison\n",
    ")\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Access the logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully from refactored structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d2b7f",
   "metadata": {},
   "source": [
    "## Data Loading (Front + Side Views)\n",
    "\n",
    "**Merging front + side views**\n",
    "- Provides complementary perspectives of each exercise\n",
    "- Improves model generalization\n",
    "\n",
    "**Data structure:** Each sample is a tuple `(exercise_label, gei_image, subject_id)`\n",
    "- `exercise_label` (str): Exercise name (e.g., \"Dumbbell shoulder press\")\n",
    "- `gei_image` (np.ndarray): 2D grayscale image (H√óW)\n",
    "- `subject_id` (str): Volunteer identifier for subject-independent splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87b7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset size: 3142 (front: 1574, side: 1568)\n",
      "Sample tuple structure: (label:str, image:np.ndarray[H,W], subject:str) -> str (1280, 720) str\n",
      "Total unique subjects: 70\n",
      "Subject preview: ['V3', 'V31', 'V39', 'V4', 'V46', 'V47', 'V48', 'V5', 'V50', 'Volunteer #1', 'Volunteer #10', 'Volunteer #2', 'Volunteer #3', 'Volunteer #31', 'Volunteer #38', 'Volunteer #39', 'Volunteer #4', 'Volunteer #40', 'Volunteer #41', 'Volunteer #42', 'Volunteer #43', 'Volunteer #44', 'Volunteer #45', 'Volunteer #46', 'Volunteer #5', 'Volunteer #50', 'Volunteer #6', 'Volunteer #7', 'Volunteer #8', 'Volunteer #9', 'v1', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v2', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v3', 'v30', 'v31', 'v32', 'v33', 'v34', 'v35', 'v36', 'v39', 'v4', 'v46', 'v49', 'v5', 'v50', 'v6', 'v7', 'v8', 'volunteer #9']\n"
     ]
    }
   ],
   "source": [
    "# Folder paths - using the datasets folder in the project\n",
    "front_base_folder = \"D:/Graduation_Project/ai-virtual-coach/datasets/GEIs_of_rgb_front/GEIs\"\n",
    "side_base_folder = \"D:/Graduation_Project/ai-virtual-coach/datasets/GEIs_of_rgb_side/GEIs\"\n",
    "\n",
    "# Load both datasets using refactored module\n",
    "front_dataset = load_data(front_base_folder)\n",
    "side_dataset = load_data(side_base_folder)\n",
    "\n",
    "# Merge and shuffle\n",
    "dataset = front_dataset + side_dataset\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Summary\n",
    "print(f\"Merged dataset size: {len(dataset)} (front: {len(front_dataset)}, side: {len(side_dataset)})\")\n",
    "\n",
    "if len(dataset) > 0:\n",
    "    sample = dataset[0]\n",
    "    print(f\"Sample tuple structure: (label:str, image:np.ndarray[H,W], subject:str) -> {type(sample[0]).__name__} {sample[1].shape} {type(sample[2]).__name__}\")\n",
    "\n",
    "subjects = get_subjects_identities(dataset)\n",
    "subject_count = len(subjects)\n",
    "print(f'Total unique subjects: {subject_count}')\n",
    "print(f'Subject preview: {subjects}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac6ff5",
   "metadata": {},
   "source": [
    "## Train Baseline Models (7 Backbones √ó 10 Runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfce0b",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "**What happens in each run:**\n",
    "1. **Data split:** Subject-independent train/test split (70/30 ratio)\n",
    "2. **Model creation:** Build transfer learning model with frozen backbone\n",
    "3. **Phase 1 training:** Train classification head (10 epochs, lr=0.001)\n",
    "4. **Phase 2 training:** Fine-tune backbone layers (10 epochs, lr=0.0001)\n",
    "5. **Evaluation:** Test on held-out subjects\n",
    "6. **Save results:** Confusion matrix, learning curves, metrics\n",
    "\n",
    "**Expected duration:** ~10-15 minutes per backbone (depends on GPU)\n",
    "\n",
    "**Progress tracking:**\n",
    "- Outer progress bar: Backbones (7 total)\n",
    "- Inner progress bar: Runs per backbone (10 runs each)\n",
    "- Logs: Real-time accuracy updates\n",
    "\n",
    "‚ö†Ô∏è **Note:** This cell will take several hours to complete all 7 backbones √ó 10 runs = 70 training runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONES_TO_TEST = [\n",
    "    'efficientnet_b0',\n",
    "    'efficientnet_b2',\n",
    "    'efficientnet_b3',\n",
    "    'resnet50',\n",
    "    'vgg16',\n",
    "    'mobilenet_v2',\n",
    "    'mobilenet_v3_large',\n",
    "]\n",
    "\n",
    "N_RUNS = 10\n",
    "TEST_RATIO = 0.3\n",
    "\n",
    "all_backbone_summaries = {}\n",
    "\n",
    "# Outer progress bar for backbones\n",
    "for bb in tqdm(BACKBONES_TO_TEST, desc=\"Backbones\", position=0):\n",
    "    logger.info(\"\\n\" + \"#\"*72)\n",
    "    logger.info(f\"Benchmarking backbone: {bb}\")\n",
    "    logger.info(\"#\"*72)\n",
    "    \n",
    "    # Create results folder for this backbone (using refactored utility)\n",
    "    RESULTS_FOLDER, RUN_INDEX = setup_results_folder_for_backbone(bb, base_results_dir='experiments/exer_recog/results/exp_01_baseline')\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Inner progress bar for runs\n",
    "    for run_idx in tqdm(range(N_RUNS), desc=f\"{bb} runs\", position=1, leave=False):\n",
    "        logger.info(f\"\\n‚ñ∂ [{bb}] Run {run_idx+1}/{N_RUNS} starting...\")\n",
    "        \n",
    "        try:\n",
    "            # Using refactored train_one_run from src.scripts.experiment_1\n",
    "            res = train_one_run(\n",
    "                run_idx=run_idx,\n",
    "                dataset=dataset,\n",
    "                test_ratio=TEST_RATIO,\n",
    "                img_size=224,\n",
    "                num_classes=15,\n",
    "                results_folder=RESULTS_FOLDER,\n",
    "                backbone=bb\n",
    "            )\n",
    "            all_results.append(res)\n",
    "            logger.info(f\"  ‚úì Completed: train_acc={res['train_acc']:.4f}  test_acc={res['test_acc']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"  ‚úó Run {run_idx} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save summary for this backbone (using refactored utility)\n",
    "    if all_results:\n",
    "        summary_path, json_path = save_experiment_summary(all_results, RESULTS_FOLDER, RUN_INDEX, TEST_RATIO)\n",
    "        \n",
    "        # Compute statistics\n",
    "        test_accs = [r['test_acc'] for r in all_results]\n",
    "        all_backbone_summaries[bb] = {\n",
    "            'mean_test_acc': np.mean(test_accs),\n",
    "            'std_test_acc': np.std(test_accs),\n",
    "            'min_test_acc': np.min(test_accs),\n",
    "            'max_test_acc': np.max(test_accs),\n",
    "            'num_runs': len(all_results)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"\\n{bb} Summary:\")\n",
    "        logger.info(f\"  Mean Test Acc: {all_backbone_summaries[bb]['mean_test_acc']:.4f} ¬± {all_backbone_summaries[bb]['std_test_acc']:.4f}\")\n",
    "\n",
    "# Display final comparison\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"FINAL BACKBONE COMPARISON\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame(all_backbone_summaries).T\n",
    "comparison_df = comparison_df.sort_values('mean_test_acc', ascending=False)\n",
    "logger.info(\"\\n\" + comparison_df.to_string())\n",
    "\n",
    "# Save comparison\n",
    "os.makedirs('experiments/exer_recog/results/exp_01_baseline', exist_ok=True)\n",
    "comparison_df.to_csv('experiments/exer_recog/results/exp_01_baseline/backbone_comparison.csv')\n",
    "logger.info(\"\\nComparison saved to: experiments/exer_recog/results/exp_01_baseline/backbone_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ea837",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE BACKBONE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Count model parameters (NO training needed!)\n",
    "backbones_to_analyze = [\n",
    "    'efficientnet_b0',\n",
    "    'efficientnet_b2',\n",
    "    'efficientnet_b3',\n",
    "    'resnet50',\n",
    "    'vgg16',\n",
    "    'mobilenet_v2',\n",
    "    'mobilenet_v3_large',\n",
    "]\n",
    "\n",
    "print(\"\\nStep 1: Counting model parameters...\")\n",
    "params_df = get_all_model_parameters(backbones_to_analyze, img_size=224, num_classes=15)\n",
    "\n",
    "print(\"\\nModel Parameter Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(params_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 2: Load training results (using refactored utility)\n",
    "print(\"\\nStep 2: Loading training results...\")\n",
    "backbone_results = load_backbone_results_with_config(results_base_dir='experiments/exer_recog/results/exp_01_baseline')\n",
    "\n",
    "# Step 3: Generate comparisons (using refactored utility)\n",
    "print(\"\\nStep 3: Generating comprehensive comparison...\")\n",
    "os.makedirs('experiments/exer_recog/results/exp_01_baseline/comparisons', exist_ok=True)\n",
    "comparison_csv = create_comprehensive_comparison(\n",
    "    all_backbone_results=backbone_results,\n",
    "    model_params_df=params_df,\n",
    "    output_dir='experiments/exer_recog/results/exp_01_baseline/comparisons'\n",
    ")\n",
    "\n",
    "# Step 4: Statistical analysis (using refactored utility)\n",
    "print(\"\\nStep 4: Performing statistical analysis...\")\n",
    "stats_txt = generate_statistical_comparison(\n",
    "    all_backbone_results=backbone_results,\n",
    "    output_dir='experiments/exer_recog/results/exp_01_baseline/comparisons'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Analysis complete! Results saved to: experiments/exer_recog/results/exp_01_baseline/comparisons/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acc39f",
   "metadata": {},
   "source": [
    "## Post-Training Analysis\n",
    "\n",
    "This comprehensive analysis includes:\n",
    "\n",
    "### 1. Model Parameters Count\n",
    "- Total parameters (trainable + non-trainable)\n",
    "- Helps understand model complexity\n",
    "- No training required - just builds models and counts\n",
    "\n",
    "### 2. Load Training Results\n",
    "- Reads saved JSON files from all runs\n",
    "- Aggregates metrics across runs\n",
    "- Prepares data for statistical analysis\n",
    "\n",
    "### 3. Comprehensive Comparison\n",
    "- Accuracy statistics (mean, std, min, max)\n",
    "- Model parameters comparison\n",
    "- Performance vs complexity analysis\n",
    "- Saves CSV report\n",
    "\n",
    "### 4. Statistical Analysis\n",
    "- Paired comparisons between backbones\n",
    "- Confidence intervals\n",
    "- Statistical significance testing (t-tests)\n",
    "- Generates detailed text report\n",
    "\n",
    "**Output location:** `experiments/exer_recog/results/exp_01_baseline/comparisons/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df58d0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 1 Complete! üéâ\n",
    "\n",
    "### Results Summary\n",
    "All results saved to: `experiments/exer_recog/results/exp_01_baseline/`\n",
    "\n",
    "**Generated files:**\n",
    "- `backbone_comparison.csv` - Quick comparison table\n",
    "- `comparisons/comprehensive_comparison.csv` - Detailed metrics + parameters\n",
    "- `comparisons/statistical_analysis.txt` - Statistical tests\n",
    "- Individual backbone folders with run-specific results\n",
    "\n",
    "### Key Findings to Look For\n",
    "\n",
    "**Best Backbone:**\n",
    "- Highest mean test accuracy\n",
    "- Lowest standard deviation (most reliable)\n",
    "- Balance between performance and parameter count\n",
    "\n",
    "**Training Stability:**\n",
    "- Which backbones converge fastest?\n",
    "- Which show most consistent results across runs?\n",
    "\n",
    "**Model Efficiency:**\n",
    "- Accuracy per million parameters\n",
    "- Inference speed (can measure separately)\n",
    "\n",
    "### Next Steps\n",
    "1. **Review comprehensive comparison** in `comparisons/` folder\n",
    "2. **Run Experiment 2** (`02_progressive.ipynb`) to test improved training strategy\n",
    "3. **Compare experiments** using `99_comparison.ipynb`\n",
    "\n",
    "This baseline establishes the performance floor - Experiment 2 should exceed these results with progressive training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
